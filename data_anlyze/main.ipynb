{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56778a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c94b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/v7b0ncfx1rz2xq3ffw1jfqp80000gn/T/ipykernel_25841/863391539.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path, encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "csv_path = '/Users/caopengbo/Documents/code/clean_item/property_clusters_output_secondary.csv'\n",
    "df = pd.read_csv(csv_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591a2e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ•°æ®è¡Œæ•°: 339519\n",
      "è¿‡æ»¤åæ•°æ®è¡Œæ•°: 120613\n",
      "cluster_idæ€»å…±æœ‰ 13745 ä¸ªç±»åˆ«\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df[(df['cluster_id'] != 'Others') & (df['cluster_total_frequency'] >= 10)]\n",
    "print(f'åŸå§‹æ•°æ®è¡Œæ•°: {len(df)}')\n",
    "print(f'è¿‡æ»¤åæ•°æ®è¡Œæ•°: {len(filtered_df)}')\n",
    "#\n",
    "cluster_count = filtered_df['cluster_id'].nunique()\n",
    "print(f'cluster_idæ€»å…±æœ‰ {cluster_count} ä¸ªç±»åˆ«')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af669234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¯ä¸ªcluster_idä¸­countæœ€é«˜çš„è¡Œæ•°: 13745\n"
     ]
    }
   ],
   "source": [
    "#åªä¿ç•™æ¯ä¸ªcluster_idä¸­countæœ€é«˜çš„è¡Œ-ä¹Ÿå°±æ˜¯ç”¨æœ€é«˜é¢‘è¯æ¥ä½œä¸ºè¯¥cluster_idçš„ä»£è¡¨\n",
    "highest_df = filtered_df.loc[filtered_df.groupby('cluster_id')['cluster_total_frequency'].idxmax()]\n",
    "print(f'æ¯ä¸ªcluster_idä¸­countæœ€é«˜çš„è¡Œæ•°: {len(highest_df)}')\n",
    "#ä¿å­˜ä»£è¡¨è¯æ–‡ä»¶\n",
    "\n",
    "output_highest_path = os.path.join(os.path.dirname(csv_path), 'highest_property_clusters_output_secondary.csv')\n",
    "\n",
    "highest_df.to_csv(output_highest_path, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6c8ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å®Œæ•´Propertyå€¼é¢‘æ¬¡ï¼ˆå‰10ä¸ªï¼‰===\n",
      "property\n",
      "Thickness                        1\n",
      "Transport energy gap value       1\n",
      "Intensity Ratio DÂ°X/FXA          1\n",
      "ESR peak-to-peak linewidth       1\n",
      "Energy level scheme              1\n",
      "Vibrational Bands                1\n",
      "Moth-eye structure morphology    1\n",
      "Conversion depth                 1\n",
      "Internal loss (Î±i)               1\n",
      "Triplet State Lifetime           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== ä¸»è¦æœ¯è¯­é¢‘æ¬¡ï¼ˆå»æ‹¬å·å†…å®¹ï¼‰===\n",
      "'Composition': 33\n",
      "'Thickness': 13\n",
      "'Morphology': 13\n",
      "'Surface Morphology': 11\n",
      "'Interlayer Spacing': 10\n",
      "'Photocurrent': 9\n",
      "'Crystal Quality': 8\n",
      "'Crystal Structure': 7\n",
      "'Stability': 7\n",
      "'Stoichiometry': 7\n",
      "\n",
      "=== æ ¸å¿ƒå…³é”®è¯é¢‘æ¬¡ ===\n",
      "energy: 679\n",
      "peak: 588\n",
      "emission: 425\n",
      "density: 409\n",
      "band: 399\n",
      "current: 375\n",
      "concentration: 340\n",
      "ratio: 333\n",
      "temperature: 332\n",
      "surface: 304\n",
      "intensity: 300\n",
      "presence: 298\n",
      "efficiency: 295\n",
      "structure: 250\n",
      "thickness: 248\n"
     ]
    }
   ],
   "source": [
    "# ç»Ÿè®¡filtered_dfä¸­propertyåˆ—æ‰€æœ‰å…³é”®è¯çš„è¯é¢‘\n",
    "# ç®€åŒ–ç‰ˆæœ¬ - åªç»Ÿè®¡æœ€é‡è¦çš„ä¿¡æ¯\n",
    "import re\n",
    "from collections import Counter\n",
    "def simple_property_analysis(df, column_name='property'):\n",
    "    \n",
    "    # 1. å®Œæ•´å€¼ç»Ÿè®¡\n",
    "    print(\"=== å®Œæ•´Propertyå€¼é¢‘æ¬¡ï¼ˆå‰10ä¸ªï¼‰===\")\n",
    "    print(df[column_name].value_counts().head(10))\n",
    "    print()\n",
    "    \n",
    "    # 2. ä¸»è¦æœ¯è¯­ç»Ÿè®¡ï¼ˆå»æ‹¬å·ï¼‰\n",
    "    print(\"=== ä¸»è¦æœ¯è¯­é¢‘æ¬¡ï¼ˆå»æ‹¬å·å†…å®¹ï¼‰===\")\n",
    "    main_terms = []\n",
    "    for prop in df[column_name].dropna():\n",
    "        # å»é™¤æ‹¬å·å†…å®¹ï¼Œæ¸…ç†ç©ºæ ¼\n",
    "        clean_term = re.sub(r'\\([^)]*\\)', '', str(prop)).strip()\n",
    "        clean_term = re.sub(r'\\s+', ' ', clean_term)  # æ ‡å‡†åŒ–ç©ºæ ¼\n",
    "        if clean_term:\n",
    "            main_terms.append(clean_term)\n",
    "    \n",
    "    main_counts = Counter(main_terms)\n",
    "    for term, count in main_counts.most_common(10):\n",
    "        print(f\"'{term}': {count}\")\n",
    "    print()\n",
    "    \n",
    "    # 3. å…³é”®è¯ç»Ÿè®¡\n",
    "    print(\"=== æ ¸å¿ƒå…³é”®è¯é¢‘æ¬¡ ===\")\n",
    "    keywords = []\n",
    "    for prop in df[column_name].dropna():\n",
    "        words = re.findall(r'[A-Za-z]{3,}', str(prop))  # æå–3ä¸ªå­—æ¯ä»¥ä¸Šçš„è¯\n",
    "        keywords.extend([w.lower() for w in words])\n",
    "    \n",
    "    # è¿‡æ»¤å¸¸è§è¯\n",
    "    stop_words = {'from', 'and', 'the', 'inferred', 'implied'}\n",
    "    filtered_keywords = [w for w in keywords if w not in stop_words]\n",
    "    \n",
    "    keyword_counts = Counter(filtered_keywords)\n",
    "    for word, count in keyword_counts.most_common(15):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "    return main_counts, keyword_counts\n",
    "\n",
    "# è¿è¡Œç®€åŒ–åˆ†æ\n",
    "main_counts, keyword_counts=simple_property_analysis(highest_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7ad1afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸»è¦æœ¯è¯­ä¿å­˜åˆ°: /Users/caopengbo/Documents/code/clean_item/main_terms_results.csv\n",
      "å…³é”®è¯ä¿å­˜åˆ°: /Users/caopengbo/Documents/code/clean_item/keywords_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ä¿å­˜ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„CSVæ–‡ä»¶\n",
    "base_path = os.path.dirname(csv_path)\n",
    "\n",
    "# ä¿å­˜ä¸»è¦æœ¯è¯­\n",
    "main_terms_path = os.path.join(base_path, 'main_terms_results.csv')\n",
    "with open(main_terms_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Main Term,Count\\n\")\n",
    "    for term, count in main_counts.items():\n",
    "        f.write(f'\"{term}\",{count}\\n')\n",
    "\n",
    "# ä¿å­˜å…³é”®è¯\n",
    "keywords_path = os.path.join(base_path, 'keywords_results.csv')\n",
    "with open(keywords_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Keyword,Count\\n\")\n",
    "    for word, count in keyword_counts.items():\n",
    "        f.write(f\"{word},{count}\\n\")\n",
    "\n",
    "print(f\"ä¸»è¦æœ¯è¯­ä¿å­˜åˆ°: {main_terms_path}\")\n",
    "print(f\"å…³é”®è¯ä¿å­˜åˆ°: {keywords_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce76590",
   "metadata": {},
   "source": [
    "ä¸Šè¿°æ˜¯å¯¹filtered_dfçš„å±æ€§åˆ—è¿›è¡Œçš„ç®€å•åˆ†æï¼ˆç»Ÿè®¡ä¸“ä¸šæœ¯è¯­å³å»æ‰æ‹¬å·çš„é¢‘æ¬¡ï¼Œæ ¸å¿ƒå…³é”®è¯çš„é¢‘æ¬¡ï¼‰ï¼Œç»“æœå·²ä¿å­˜ä¸ºCSVæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c14a2757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹highest_dfçš„é«˜é¢‘è¯æ±‡èšç±»ï¼Œç›¸ä¼¼åº¦è¾ƒä½ï¼Œæ—¨åœ¨ä¸ºå…¶æ„å»ºä¸Šä¸€çº§èšç±»\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "def perform_property_clustering(df, column_name='property', similarity_threshold=0.3, min_community_size=2):\n",
    "    \"\"\"\n",
    "    å¯¹propertyåˆ—è¿›è¡Œèšç±»åˆ†æ\n",
    "    \n",
    "    Args:\n",
    "        df: åŒ…å«propertyåˆ—çš„DataFrame\n",
    "        column_name: è¦èšç±»çš„åˆ—å\n",
    "        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼\n",
    "        min_community_size: æœ€å°ç°‡å¤§å°\n",
    "    \n",
    "    Returns:\n",
    "        èšç±»ç»“æœå­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ¤– å¼€å§‹å¯¹{column_name}åˆ—è¿›è¡Œèšç±»...\")\n",
    "    print(f\"  - ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}\")\n",
    "    print(f\"  - æœ€å°ç°‡å¤§å°: {min_community_size}\")\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    properties = df[column_name].dropna().tolist()\n",
    "    print(f\"  - å¾…èšç±»çš„å±æ€§æ•°é‡: {len(properties)}\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹å¹¶è®¡ç®—åµŒå…¥\n",
    "    print(\"  - æ­£åœ¨åŠ è½½è¯­è¨€æ¨¡å‹...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    print(\"  - æ­£åœ¨è®¡ç®—å¥å­åµŒå…¥...\")\n",
    "    embeddings = model.encode(properties, convert_to_tensor=True)\n",
    "    \n",
    "    # æ‰§è¡Œç¤¾åŒºæ£€æµ‹èšç±»\n",
    "    print(\"  - æ­£åœ¨æ‰§è¡Œèšç±»...\")\n",
    "    clusters = util.community_detection(\n",
    "        embeddings,\n",
    "        min_community_size=min_community_size,\n",
    "        threshold=similarity_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… èšç±»å®Œæˆï¼å…±æ‰¾åˆ° {len(clusters)} ä¸ªç°‡ã€‚\")\n",
    "    \n",
    "    # å¤„ç†èšç±»ç»“æœ\n",
    "    clustered_indices = set()\n",
    "    clustered_results = []\n",
    "    \n",
    "    for i, cluster_indices in enumerate(clusters):\n",
    "        cluster_members = []\n",
    "        for idx in cluster_indices:\n",
    "            property_name = properties[idx]\n",
    "            # ä»åŸå§‹DataFrameä¸­è·å–è¯¥propertyçš„é¢‘æ¬¡ä¿¡æ¯\n",
    "            count = len(df[df[column_name] == property_name])\n",
    "            cluster_members.append({\n",
    "                'property': property_name,\n",
    "                'count': count\n",
    "            })\n",
    "        \n",
    "        clustered_results.append({\n",
    "            'cluster_id': f\"cluster_{i+1}\",\n",
    "            'size': len(cluster_members),\n",
    "            'members': cluster_members\n",
    "        })\n",
    "        clustered_indices.update(cluster_indices)\n",
    "    \n",
    "    # è¯†åˆ«æœªèšç±»çš„é¡¹ç›®\n",
    "    all_indices = set(range(len(properties)))\n",
    "    unclustered_indices = all_indices - clustered_indices\n",
    "    unclustered_items = []\n",
    "    \n",
    "    for idx in unclustered_indices:\n",
    "        property_name = properties[idx]\n",
    "        count = len(df[df[column_name] == property_name])\n",
    "        unclustered_items.append({\n",
    "            'property': property_name,\n",
    "            'count': count\n",
    "        })\n",
    "    \n",
    "    print(f\"  - {len(clustered_indices)} ä¸ªå±æ€§è¢«èšç±»\")\n",
    "    print(f\"  - {len(unclustered_indices)} ä¸ªå±æ€§æœªè¢«èšç±»\")\n",
    "    \n",
    "    return {\n",
    "        'clustered_results': clustered_results,\n",
    "        'unclustered_items': unclustered_items,\n",
    "        'total_clusters': len(clusters),\n",
    "        'clustered_count': len(clustered_indices),\n",
    "        'unclustered_count': len(unclustered_indices)\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3de7471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤– å¼€å§‹å¯¹propertyåˆ—è¿›è¡Œèšç±»...\n",
      "  - ç›¸ä¼¼åº¦é˜ˆå€¼: 0.5\n",
      "  - æœ€å°ç°‡å¤§å°: 2\n",
      "  - å¾…èšç±»çš„å±æ€§æ•°é‡: 13744\n",
      "  - æ­£åœ¨åŠ è½½è¯­è¨€æ¨¡å‹...\n",
      "  - æ­£åœ¨è®¡ç®—å¥å­åµŒå…¥...\n",
      "  - æ­£åœ¨æ‰§è¡Œèšç±»...\n",
      "âœ… èšç±»å®Œæˆï¼å…±æ‰¾åˆ° 1650 ä¸ªç°‡ã€‚\n",
      "  - 13438 ä¸ªå±æ€§è¢«èšç±»\n",
      "  - 306 ä¸ªå±æ€§æœªè¢«èšç±»\n"
     ]
    }
   ],
   "source": [
    "# å¯¹highest_dfçš„é«˜é¢‘è¯æ±‡èšç±»ï¼Œç›¸ä¼¼åº¦è¾ƒä½ï¼Œæ—¨åœ¨ä¸ºå…¶æ„å»ºä¸Šä¸€çº§èšç±»\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# è®¡ç®—å¥å­åµŒå…¥\n",
    "embeddings = model.encode(highest_df['property'].tolist(), convert_to_tensor=True)\n",
    "#ä½¿ç”¨ç¤¾åŒºç®—æ³•èšç±»\n",
    "results = perform_property_clustering(highest_df, column_name='property', similarity_threshold=0.5, min_community_size=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e4119",
   "metadata": {},
   "source": [
    "0.3çš„ç›¸ä¼¼åº¦å¯ä»¥èšç±»åˆ°700ä¸ªï¼Œ\n",
    "0.5çš„ç›¸ä¼¼åº¦å¯ä»¥èšç±»åˆ°1650ä¸ªï¼Œæ‰€ä»¥æš‚å®šä¸€ä¸ªå€¼ï¼ˆå¯ä»¥ä¸º1650ä¹Ÿå¯ä»¥æ˜¯å…¶ä»–å€¼ï¼Œèšç±»åç”¨å¤§è¯­è¨€æ¨¡å‹ç»Ÿä¸€å‘½åï¼‰\n",
    "ç„¶åå¯¹æœ€é«˜é¢‘çš„1650ä¸ªè¿›è¡Œèšç±»ï¼Œå¾—åˆ°çš„ç»“æœæ˜¯1650ä¸ªèšç±»ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02b20742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœªèšç±»é¡¹ç›®ä¿å­˜åˆ°: high_unclustered_properties.csv\n",
      "èšç±»ç»“æœä¿å­˜åˆ°: high_property_clusters_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ä¿å­˜é«˜é¢‘è¯è’¸é¦ç»“æœ\n",
    "\n",
    "def save_clustering_results(results, base_path):\n",
    "    \"\"\"ä¿å­˜èšç±»ç»“æœåˆ°CSVæ–‡ä»¶\"\"\"\n",
    "    \n",
    "    # ä¿å­˜èšç±»ç»“æœ\n",
    "    clusters_data = []\n",
    "    for cluster in results['clustered_results']:\n",
    "        for member in cluster['members']:\n",
    "            clusters_data.append({\n",
    "                'cluster_id': cluster['cluster_id'],\n",
    "                'property': member['property'],\n",
    "                'count': member['count'],\n",
    "                'cluster_size': cluster['size']\n",
    "            })\n",
    "    \n",
    "    clusters_df = pd.DataFrame(clusters_data)\n",
    "    clusters_path = os.path.join(base_path, 'high_property_clusters_results.csv')\n",
    "    clusters_df.to_csv(clusters_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    # ä¿å­˜æœªèšç±»é¡¹ç›®\n",
    "    if results['unclustered_items']:\n",
    "        unclustered_df = pd.DataFrame(results['unclustered_items'])\n",
    "        unclustered_path = os.path.join(base_path, 'high_unclustered_properties.csv')\n",
    "        unclustered_df.to_csv(unclustered_path, index=False, encoding='utf-8')\n",
    "        print(f\"æœªèšç±»é¡¹ç›®ä¿å­˜åˆ°: {unclustered_path}\")\n",
    "    \n",
    "    print(f\"èšç±»ç»“æœä¿å­˜åˆ°: {clusters_path}\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "base_path = os.path.dirname('data_anlyze')\n",
    "save_clustering_results(results, base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7034d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_extra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
