# -*- coding: utf-8 -*-
"""
ä¸€ä¸ªé«˜çº§è„šæœ¬ï¼Œç”¨äºå¯¹å¸¦æœ‰é¢‘æ¬¡ç»Ÿè®¡çš„å±æ€§åˆ—è¡¨è¿›è¡Œä¸¤è½®è¯­ä¹‰èšç±»å’Œç­›é€‰ã€‚

è¯¥è„šæœ¬é€šè¿‡ä¸€ä¸ªä¸»ç±» `PropertyClusterAnalyzer` æ¥ç»„ç»‡æ•´ä¸ªæµç¨‹ï¼Œä½¿å…¶æ›´å…·
æ¨¡å—åŒ–å’Œå¯é‡ç”¨æ€§ã€‚

å·¥ä½œæµç¨‹:
1.  ä»å¤æ‚çš„ JSON ç»“æ„ä¸­åŠ è½½å±æ€§åŠå…¶é¢‘æ¬¡ã€‚
2.  ä½¿ç”¨ SBERT æ¨¡å‹ä¸ºæ‰€æœ‰å±æ€§ç”Ÿæˆè¯­ä¹‰å‘é‡ã€‚
3.  æ‰§è¡Œç¬¬ä¸€è½®ç²—ç²’åº¦èšç±»ã€‚
4.  åœ¨æ¯ä¸ªç²—ç²’åº¦ç°‡å†…éƒ¨ï¼Œæ‰§è¡Œç¬¬äºŒè½®ç»†ç²’åº¦èšç±»ã€‚
5.  æ ¹æ®é¢‘ç‡é˜ˆå€¼ï¼Œåœ¨ç»†ç²’åº¦ç°‡å†…éƒ¨è¿›è¡Œåˆå¹¶ã€‚
6.  å¤„ç†æ‰€æœ‰èšç±»å’Œæœªèšç±»é¡¹ï¼Œå¹¶å°†æœ€ç»ˆç»“æœä¿å­˜åˆ° CSV æ–‡ä»¶ã€‚
"""
import json
import torch
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import csv
from typing import List, Dict, Any

class PropertyClusterAnalyzer:
    """
    ä¸€ä¸ªç”¨äºå¯¹å±æ€§åˆ—è¡¨è¿›è¡Œä¸¤è½®è¯­ä¹‰èšç±»åˆ†æçš„ç±»ã€‚
    """

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–åˆ†æå™¨ã€‚

        Args:
            config (Dict[str, Any]): åŒ…å«æ‰€æœ‰é…ç½®å‚æ•°çš„å­—å…¸ã€‚
        """
        self.config = config
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"INFO: ä½¿ç”¨è®¾å¤‡ '{self.device}'")
        self.model = SentenceTransformer(config['sbert_model'], device=self.device)
        self.term_counts = {}
        self.terms_to_cluster = []
        self.unclustered_items = []

    def _load_data(self):
        """ä»è¾“å…¥JSONæ–‡ä»¶ä¸­åŠ è½½å’Œå‡†å¤‡æ•°æ®ã€‚"""
        print(f"ğŸ”„ æ­¥éª¤ 1: æ­£åœ¨ä» '{self.config['input_json_path']}' åŠ è½½æ•°æ®...")
        try:
            with open(self.config['input_json_path'], "r", encoding="utf-8") as f:
                data = json.load(f)

            field_key = self.config['field_to_analyze']
            if field_key not in data or 'sorted_by_frequency' not in data[field_key]:
                raise ValueError(f"JSON æ–‡ä»¶ä¸­æœªæ‰¾åˆ° '{field_key}' æˆ–å…¶ä¸‹çš„ 'sorted_by_frequency' é”®")

            property_list = data[field_key]['sorted_by_frequency']
            df = pd.DataFrame(property_list, columns=['property', 'count'])
            
            self.terms_to_cluster = df['property'].tolist()
            self.term_counts = df.set_index('property')['count'].to_dict()

            print(f"  - æˆåŠŸä» '{field_key}' åŠ è½½äº† {len(self.terms_to_cluster)} ä¸ªå”¯ä¸€å±æ€§ã€‚")
            print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
        except FileNotFoundError:
            print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ '{self.config['input_json_path']}' æœªæ‰¾åˆ°ã€‚")
            exit()
        except (json.JSONDecodeError, ValueError) as e:
            print(f"é”™è¯¯: è§£æ JSON æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            exit()

    def _perform_primary_clustering(self, term_embeddings):
        """æ‰§è¡Œç¬¬ä¸€è½®åˆæ­¥èšç±»ã€‚"""
        print("\nğŸ¤– æ­¥éª¤ 2: æ­£åœ¨æ‰§è¡Œç¬¬ä¸€è½®åˆæ­¥èšç±»...")
        print(f"  - å‚æ•°: ç›¸ä¼¼åº¦é˜ˆå€¼={self.config['primary_cluster_threshold']}, æœ€å°ç°‡å¤§å°={self.config['min_community_size']}")
        
        clusters = util.community_detection(
            term_embeddings,
            min_community_size=self.config['min_community_size'],
            threshold=self.config['primary_cluster_threshold']
        )
        print(f"âœ… åˆæ­¥èšç±»å®Œæˆï¼å…±æ‰¾åˆ° {len(clusters)} ä¸ªç°‡ã€‚")
        
        # å¤„ç†ç»“æœå¹¶è¯†åˆ«æœªèšç±»é¡¹
        clustered_indices = set()
        clustered_results = []
        for i, cluster_indices in enumerate(clusters):
            cluster_members = [{'property': self.terms_to_cluster[idx], 'count': self.term_counts.get(self.terms_to_cluster[idx], 0)} for idx in cluster_indices]
            clustered_results.append({
                'cluster_id': f"primary_{i+1}",
                'members': cluster_members
            })
            clustered_indices.update(cluster_indices)
        
        all_indices = set(range(len(self.terms_to_cluster)))
        unclustered_indices = all_indices - clustered_indices
        self.unclustered_items = [{'property': self.terms_to_cluster[idx], 'count': self.term_counts.get(self.terms_to_cluster[idx], 0)} for idx in unclustered_indices]
        
        print(f"  - {len(clustered_indices)} ä¸ªå±æ€§è¢«åˆæ­¥èšç±»ã€‚")
        print(f"  - {len(unclustered_indices)} ä¸ªå±æ€§åœ¨ç¬¬ä¸€è½®æœªè¢«èšç±»ã€‚")
        
        return clustered_results

    def _perform_secondary_clustering(self, primary_clusters: List[Dict]) -> List[Dict]:
        """åœ¨æ¯ä¸ªä¸»ç°‡å†…æ‰§è¡Œç¬¬äºŒè½®ç²¾èšç±»å’Œåˆå¹¶ã€‚"""
        print("\nğŸ”¬ æ­¥éª¤ 3: æ­£åœ¨æ‰§è¡Œç¬¬äºŒè½®ç²¾èšç±»ä¸åˆå¹¶...")
        final_clusters = []
        
        for p_cluster in primary_clusters:
            members = [member['property'] for member in p_cluster['members']]
            if len(members) <= 1:
                # å¦‚æœä¸»ç°‡æˆå‘˜è¿‡å°‘ï¼Œç›´æ¥è§†ä¸ºä¸€ä¸ªæœ€ç»ˆç°‡
                final_clusters.append({
                    'sub_cluster_members': members,
                    'sub_cluster_total_frequency': sum(m['count'] for m in p_cluster['members'])
                })
                continue

            member_embeddings = self.model.encode(members, convert_to_tensor=True, show_progress_bar=False)
            sub_clusters_indices = util.community_detection(
                member_embeddings,
                min_community_size=1,
                threshold=self.config['secondary_cluster_threshold']
            )
            print(f"  - åœ¨ç°‡ {p_cluster['cluster_id']} å†…éƒ¨æ‰¾åˆ° {len(sub_clusters_indices)} ä¸ªå­ç°‡ã€‚")

            middle_clusters = []
            for indices in sub_clusters_indices:
                sub_members = [members[idx] for idx in indices]
                sub_freq = sum(self.term_counts.get(m, 0) for m in sub_members)
                middle_clusters.append({
                    'sub_cluster_members': sub_members,
                    'sub_cluster_total_frequency': sub_freq
                })
            
            if not middle_clusters:
                continue

            # æŒ‰é¢‘ç‡æ’åºå¹¶åˆå¹¶ä½é¢‘å­ç°‡
            sorted_middle = sorted(middle_clusters, key=lambda x: x['sub_cluster_total_frequency'], reverse=True)
            base_cluster = sorted_middle[0]
            retained_sub_clusters = []
            
            freq_threshold_value = self.config['file_count_for_threshold'] * self.config['frequency_threshold_percent']

            for i in range(1, len(sorted_middle)):
                sub_cluster = sorted_middle[i]
                if sub_cluster['sub_cluster_total_frequency'] < freq_threshold_value:
                    base_cluster['sub_cluster_members'].extend(sub_cluster['sub_cluster_members'])
                    base_cluster['sub_cluster_total_frequency'] += sub_cluster['sub_cluster_total_frequency']
                else:
                    retained_sub_clusters.append(sub_cluster)
            
            retained_sub_clusters.insert(0, base_cluster)
            final_clusters.extend(retained_sub_clusters)
            
        print(f"âœ… ç¬¬äºŒè½®å¤„ç†å®Œæˆï¼å…±å½¢æˆ {len(final_clusters)} ä¸ªæœ€ç»ˆç°‡ã€‚")
        return final_clusters

    def _save_results_to_csv(self, final_clusters: List[Dict]):
        """å°†æœ€ç»ˆç»“æœä¿å­˜åˆ°CSVæ–‡ä»¶ã€‚"""
        output_path = self.config['output_csv_path']
        print(f"\nğŸ’¾ æ­¥éª¤ 4: æ­£åœ¨å°†è¯¦ç»†ç»“æœä¿å­˜åˆ° '{output_path}'...")
        
        csv_data = [['cluster_id', 'cluster_total_frequency', 'member_count', 'property', 'count']]
        cluster_id_counter = 1
        
        # å¤„ç†èšç±»åå½¢æˆçš„ç°‡
        sorted_clusters = sorted(final_clusters, key=lambda x: x['sub_cluster_total_frequency'], reverse=True)
        for cluster in sorted_clusters:
            total_freq = cluster['sub_cluster_total_frequency']
            member_count = len(cluster['sub_cluster_members'])
            for member in cluster['sub_cluster_members']:
                csv_data.append([
                    cluster_id_counter,
                    total_freq,
                    member_count,
                    member,
                    self.term_counts.get(member, 0)
                ])
            cluster_id_counter += 1
            
        # å¤„ç†æœªèšç±»çš„é¡¹
        others_group = []
        freq_threshold_value = self.config['file_count_for_threshold'] * self.config['frequency_threshold_percent']
        
        for item in self.unclustered_items:
            if item['count'] < freq_threshold_value:
                others_group.append(item)
            else:
                csv_data.append([
                    cluster_id_counter,
                    item['count'],
                    1,
                    item['property'],
                    item['count']
                ])
                cluster_id_counter += 1
        
        if others_group:
            total_others_freq = sum(item['count'] for item in others_group)
            for item in others_group:
                csv_data.append([
                    'Others',
                    total_others_freq,
                    len(others_group),
                    item['property'],
                    item['count']
                ])

        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(csv_data)
            
        print(f"âœ… ç»“æœå·²æˆåŠŸä¿å­˜ã€‚")
        # å‡å»æœªèšç±»ä½†æ»¡è¶³é¢‘ç‡é˜ˆå€¼çš„é¡¹å’Œ'Others'
        new_property_count = cluster_id_counter - 1 - len([item for item in self.unclustered_items if item['count'] >= freq_threshold_value])
        if others_group:
            new_property_count -=1 # å¦‚æœæœ‰Othersç»„ï¼ŒIDè®¡æ•°å™¨ä¼šå¤šä¸€ä¸ª
        
        print(f"\nğŸ‰ æ‰€æœ‰æµç¨‹å®Œæˆï¼å…±å®šä¹‰äº† {new_property_count} ä¸ªæ ¸å¿ƒå±æ€§ç°‡ã€‚")


    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„èšç±»åˆ†ææµç¨‹ã€‚"""
        self._load_data()
        
        print("\nğŸ§  æ­£åœ¨ä¸ºæ‰€æœ‰å±æ€§ç”Ÿæˆè¯­ä¹‰å‘é‡...")
        term_embeddings = self.model.encode(self.terms_to_cluster, convert_to_tensor=True, show_progress_bar=True)
        print(f"âœ… å·²ä¸º {len(term_embeddings)} ä¸ªå±æ€§ç”Ÿæˆå‘é‡ã€‚")
        
        primary_clusters = self._perform_primary_clustering(term_embeddings)
        final_clusters = self._perform_secondary_clustering(primary_clusters)
        self._save_results_to_csv(final_clusters)


if __name__ == '__main__':
    # ==============================================================================
    # é…ç½®åŒºåŸŸ
    # ==============================================================================
    CONFIG = {
        # --- æ–‡ä»¶è·¯å¾„ ---
        "input_json_path": '/Volumes/mac_outstore/work/3-5-1000-item/extracted_fields_1000_sample.json',
        "output_csv_path": 'property_clusters_output_secondary.csv',
        
        # --- æ•°æ®å­—æ®µ ---
        "field_to_analyze": 'names_frequency', # å¯é€‰ 'names_frequency' æˆ– 'physical_forms_frequency'
        
        # --- æ¨¡å‹ä¸èšç±»å‚æ•° ---
        "sbert_model": 'all-MiniLM-L6-v2',
        "primary_cluster_threshold": 0.85,    # ç¬¬ä¸€è½®èšç±»ç›¸ä¼¼åº¦é˜ˆå€¼
        "secondary_cluster_threshold": 0.95,  # ç¬¬äºŒè½®èšç±»ç›¸ä¼¼åº¦é˜ˆå€¼
        "min_community_size": 2,              # ç¬¬ä¸€è½®èšç±»ä¸­ï¼Œä¸€ä¸ªç°‡æœ€å°‘åŒ…å«çš„æˆå‘˜æ•°é‡
        
        # --- é¢‘ç‡ç­›é€‰å‚æ•° ---
        "file_count_for_threshold": 1000,     # ç”¨äºè®¡ç®—é¢‘ç‡ç™¾åˆ†æ¯”åŸºæ•°çš„æ€»æ–‡ä»¶æ•°
        "frequency_threshold_percent": 0.01   # é¢‘ç‡ç­›é€‰é˜ˆå€¼ (ä¾‹å¦‚ 0.01 ä»£è¡¨ 1%)
    }

    # åˆ›å»ºåˆ†æå™¨å®ä¾‹å¹¶è¿è¡Œ
    analyzer = PropertyClusterAnalyzer(CONFIG)
    analyzer.run()
